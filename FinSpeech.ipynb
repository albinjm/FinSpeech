{"cells":[{"cell_type":"markdown","metadata":{"id":"_JBERAXX86Ic"},"source":["# Speech Recognition in Banking Dialogues with CRDNN and Language Models\n","\n","## Introduction\n","\n","This project focuses on building and fine-tuning a speech recognition system tailored for banking dialogues, using a Convolutional, Recurrent, and Dense Neural Network (CRDNN) coupled with Language Model (LM) support. By leveraging the HarperValleyBank (HVB) corpus, a publicly-available spoken dialog corpus annotated for banking interactions, we demonstrate the application of advanced speech recognition technologies in understanding and transcribing customer service dialogues in the financial sector.\n","\n","### Project Objective\n","\n","The main objective of this project is to enhance the accuracy and efficiency of automatic speech recognition (ASR) systems in the context of banking dialogues. By training and fine-tuning the CRDNN model with the HVB dataset and integrating it with LM support, we aim to create a robust ASR system that can accurately transcribe spoken dialogues, thereby facilitating better customer service automation and analysis in the banking industry.\n","\n","### Dataset\n","\n","- **HarperValleyBank (HVB) Corpus**: A publicly-available spoken dialog corpus designed for the banking sector. It contains annotated dialogues covering various banking-related interactions. More details can be found in the [HVB Corpus Documentation](https://arxiv.org/pdf/2010.13929.pdf).\n","\n","### Methodology\n","\n","1. **CRDNN Model Tuning**: Starting with a pre-trained CRDNN model from SpeechBrain, we further tune the model using the HVB dataset to adapt it to banking dialogues.\n","2. **Language Model Integration**: Incorporate language models to improve the transcription accuracy by providing contextual clues that are specific to banking conversations.\n","3. **Evaluation**: Assess the performance of the fine-tuned CRDNN model, with and without LM support, on a test subset of the HVB corpus to evaluate its effectiveness in transcribing banking dialogues.\n","\n","### Libraries and Tools Used\n","\n","- **SpeechBrain**: A PyTorch-based open-source speech toolkit that provides pre-trained models and tools for ASR systems.\n","- **Hugging Face**: For accessing and deploying the SpeechBrain pre-trained CRDNN model.\n","- **Python Libraries**: `torchaudio`, `torch`, `json`, and others for data handling and model training.\n"]},{"cell_type":"markdown","metadata":{"id":"b5qc5LC2cWtS"},"source":["# Dependencies "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etpTb5AW8uM0"},"outputs":[],"source":["# !gdown 1oJh0U3g_bUx6UPX4xix2UHMVHeCE_H1y\n","!gdown 1_OXiLOL2RBsbdCb4WyQsLudYxzJxMDJr\n","!unzip -q hvb.zip\n","!mv content/data /content/\n","!rm -r /content/content\n","\n","!gdown 1a0EGlsLbXnGn1xwZoSqT0tcdAQ1L2nfd # train.py\n","!gdown 1yCmjRbxXRxfEN5LXdnE1Zpl8ZOIzdrAO # train.yaml\n","!gdown 1KHmdcLVFI9ontvGmi5J6vfaropGYuKcr # inference.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30NIWqcyAD-G"},"outputs":[],"source":["!pip install speechbrain -q\n","\n","import speechbrain as sb\n","from speechbrain.pretrained import EncoderDecoderASR\n","import json\n","import torchaudio\n","import torch\n","from torch import nn\n","from tqdm import tqdm\n","from collections import Counter\n","from IPython.display import Audio\n","from scipy.io import wavfile\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"YwgCA2Oc9h_h"},"source":["### Evaluation and Fine-Tuning of a Pretrained CRDNN Model with HVB\n","\n","The process begins by utilizing the [SpeechBrain CRDNN model pretrained on LibriSpeech](https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech), which can be sourced easily thanks to SpeechBrain's utility functions that facilitate fetching pre-trained models from HuggingFace's repository.\n","\n","Initially, the focus is on leveraging this pretrained CRDNN model for inference purposes, specifically targeting the first 500 examples from `test_manifest.json`, a file that is part of the `hvb.zip` previously acquired. Following the initial inference phase, the model undergoes fine-tuning using the HVB training dataset. The final step involves reassessing the model's performance on the test examples to observe and document any variations in its effectiveness post-fine-tuning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqTF_KNI_mGI"},"outputs":[],"source":["crdnn = EncoderDecoderASR.from_hparams(\n","    source='speechbrain/asr-crdnn-rnnlm-librispeech',\n","    savedir='asr-crdnn-rnnlm-librispeech',\n","    run_opts={'device': 'cuda'}\n",")"]},{"cell_type":"markdown","metadata":{"id":"n9Z1gOWCJCCT"},"source":["Our approach involves using manifests prepared in JSON format, designed specifically for compatibility with SpeechBrain. These manifests adhere to a structure exemplified below:\n","\n","```\n","{\n","    \"15748\": {\n","        \"wav\": \"/content/data/segments/15748.wav\",\n","        \"length\": 1.86,\n","        \"words\": \"WHAT DAY WOULD YOU LIKE FOR YOUR APPOINTMENT\"\n","    },\n","    ...\n","}\n","```\n","\n","The initial step consists of loading these manifests. Subsequently, we focus on creating a function that organizes these data entries into batches. These batches are tailored to be easily processed by our `EncoderDecoderASR` object, ensuring efficient handling and analysis of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NH2kZ0PbAsb1"},"outputs":[],"source":["TEST_SIZE = 200 # for faster processing\n","\n","with open('data/test_manifest.json', 'r') as f:\n","    test_manifest = json.load(f)\n","test_manifest = {\n","    k: v for k, v in list(test_manifest.items())[:TEST_SIZE]\n","}\n","\n","def batchify(manifest, batch_size):\n","    keys = list(manifest.keys())\n","    wav_paths = list(map(lambda x: x['wav'], manifest.values()))\n","    iterable = zip(keys, wav_paths)\n","    num_examples = len(manifest)\n","    for i in range(0, num_examples, batch_size):\n","        batch_wavs = nn.utils.rnn.pad_sequence([\n","            torchaudio.load(path)[0].squeeze(0)\n","            for path in wav_paths[i:min(i + batch_size, num_examples)]\n","        ], batch_first=True)\n","        batch_keys = keys[i:min(i + batch_size, num_examples)]\n","        batch_wav_lens = torch.tensor([\n","            manifest[key]['length'] for key in batch_keys\n","        ])\n","        batch_wav_lens = batch_wav_lens / batch_wav_lens.max()\n","        yield batch_keys, batch_wavs, batch_wav_lens"]},{"cell_type":"markdown","metadata":{"id":"tdRtje1OLiG5"},"source":["The next step is to use the pre-trained ASR model to transcribe the test examples from the HVB corpus:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YCNyfAzFh-y"},"outputs":[],"source":["true_dict = {key: test_manifest[key]['words'] for key in test_manifest}\n","\n","def inference(model, test_manifest, batch_size=8):\n","    torch.cuda.empty_cache()\n","    pred_dict = {}\n","    for keys, wavs, wav_lens in tqdm(batchify(test_manifest, batch_size), total=round(len(test_manifest) / batch_size + 0.5)):\n","        transcriptions, _ = model.transcribe_batch(wavs.to(device), wav_lens.to(device))\n","        for key, transcription in zip(keys, transcriptions):\n","            pred_dict[key] = transcription\n","    return pred_dict\n","\n","pred_dict = inference(crdnn, test_manifest)"]},{"cell_type":"markdown","metadata":{"id":"Dbnw0_ZKL9TQ"},"source":["### Determining Word Error Rate for Pretrained Model Predictions\n","\n","The code below calculates the word error rate (WER) for the predictions made by the pretrained model on the first 200 test instances in `test_manifest.json`. It processes the transcripts by splitting them into word lists to evaluate the WER.\n","\n","No new implementation is needed; simply execute the provided code to compute and review the WER for the generated results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFnB0JOfPZoj"},"outputs":[],"source":["# this data structure stores WER information we use later. \n","details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JU-xiNDdSabz"},"outputs":[],"source":["# word error rate (WER) summary using data structure we just created\n","sb.utils.edit_distance.wer_summary(details_by_utterance)"]},{"cell_type":"markdown","metadata":{"id":"c0aOgXMKSowv"},"source":["It's anticipated that the WER for this pretrained system might be quite high when tested on HVB data, potentially around or exceeding 72%. This highlights the system's limitations or mismatches with the HVB dataset, even after resampling the audio to 16kHz to align with the pretrained model's training inputs.\n","\n","ASR systems often reveal specific error patterns, which can help identify performance challenges. To understand where the pretrained system struggles with HVB data, start by examining some utterances with the highest misrecognition rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCOAzZofSoOH"},"outputs":[],"source":["def summarize(detail_dict, true_dict, pred_dict):\n","    print(f\"{detail_dict['key']}: {detail_dict['WER']}\")\n","    print(f\"\\tTrue: {true_dict[detail_dict['key']]}\")\n","    print(f\"\\tPred: {pred_dict[detail_dict['key']]}\")\n","\n","for wer_dict in sb.utils.edit_distance.top_wer_utts(details_by_utterance, 10)[0]:\n","    summarize(wer_dict, true_dict, pred_dict)"]},{"cell_type":"markdown","metadata":{"id":"RrJ-gKFXUHEf"},"source":["Seems that our predictions keep outputting the same word over and over. Let's see why."]},{"cell_type":"markdown","metadata":{"id":"LrHqM-WfzFes"},"source":["In this section of our project tutorial, we will analyze examples where the model's predictions deviate from expected outcomes. This analysis is crucial for understanding the data's nuances that may negatively impact the model's performance. Common errors to look for include:\n","\n","- Words consistently misidentified by the model across various audio samples.\n","- Accurate capture of some words, but overall transcripts that do not align well with the original audio content.\n","\n","To perform a thorough analysis, identify at least three audio files where the model's predictions are notably inaccurate. For each file, document the specific error types observed, detailing the discrepancies between the model's output and the actual audio content."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVNDEHXvWF-D"},"outputs":[],"source":["example = details_by_utterance[1]\n","summarize(example, true_dict, pred_dict)\n","Audio(test_manifest[example['key']]['wav'])"]},{"cell_type":"markdown","metadata":{"id":"3Bm4ZQZQmJU_"},"source":["## Finetuning CRDNN Model\n","\n","This section explores how the pretrained model performs on our dataset, noting its initial training was on a different dataset compared to HVB's call-center transcripts. By fine-tuning this model with HVB's training data, we aim to observe potential performance improvements. Minimal adjustments are needed, primarily to initiate the training. Experiment with training and decoding parameters to observe their effects. This hands-on exercise in fine-tuning on a new corpus using SpeechBrain offers valuable experience in ASR model development relevant to industry projects.\n","\n","All necessary components, including the training script, experiment YAML, and inference YAML, are provided. Reviewing these components and understanding the setup, especially the structure of a well-organized ML experiment YAML file, can be enlightening. Training this model for 2 epochs on Colab GPUs is expected to take about 1.5 hours.\n","\n","During training, the model will save checkpoints for later inference/testing. This setup allows you to benefit from a fine-tuned model version without completing the entire training duration. You can submit your work with the model partially trained, as long as it has undergone some fine-tuning, even if it doesn't reach the full 2 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iM2jQrwTmIw2"},"outputs":[],"source":["# this downloads the training and config files for our fine tuning setup\n","!gdown 1v_3Kl8OrUd6_1_D0ZGoYVFEuOKhZ7YMo # train.py\n","!gdown 17cQIpx5kLLMCD23EDaE0EYg2E9LPqMCF # train.yaml\n","!gdown 1CWYOD2PC97gXguW4krc9122HKAraHkYS # inference.yaml"]},{"cell_type":"markdown","metadata":{"id":"qg-TThsjy2Ul"},"source":["**Finetuning with HVB Data**\n","\n","The code below implements the finetuning process for the neural net ASR system using the HVB data. \n","\n","- `train.yaml`: This YAML config file specifies the network/ASR system architecture and training parameters such as loss functions, datasets, and more. While you can't modify the network architecture, you can adjust hyperparameters like loss function weights, learning rate, and training time.\n","- `train.py`: This file contains the main training loop for fitting the acoustic model. No modifications are needed here.\n","\n","Edit `train.yaml` with your chosen hyperparameters, then run the training loop. Record the train loss, valid loss, valid CER, and valid WER from the output in `README.txt`, including the epoch number. (Using the default `train.yaml`, we achieved training and validation loss < 1.75 after epoch 1.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpCTGMTGohrq"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","!python train.py train.yaml --batch_size=4\n","# OOM on batch_size=5"]},{"cell_type":"markdown","metadata":{"id":"QLXP3qH_AxCN"},"source":["## Evaluate your finetuned model"]},{"cell_type":"markdown","metadata":{"id":"bOM1BF3TtRB0"},"source":["To run inference, ensure compatibility with the `EncoderDecoderASR` class by using a different YAML configuration file.\n","\n","**Note**: Set your checkpoint path correctly in multiple locations to avoid issues, especially before any debugging attempts (e.g., downloading from HuggingFace).\n","\n","To enable inference, follow these steps:\n","\n","1. Determine the directory where your checkpoints are saved (usually under `./results/CRDNN_BPE_960h_LM/2602/save/{your checkpoint directory here}`).\n","2. Paste this directory into the `ckptdir` entry in the `inference.yaml` file.\n","3. Paste this directory after `ckpt_path = ` in the cell below.\n","\n","Ensure the `ckpt_path` is correctly set below and also update the path in `inference.yaml` (modify `ckptdir` to point to the desired checkpoint) before or after copying it into your checkpoint path."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3ScTzkkqMYY"},"outputs":[],"source":["ckpt_path = \"/content/results/CRDNN_BPE_960h_LM/2602/save/CKPT+2023-04-08+06-09-55+00\"\n","!cp inference.yaml {ckpt_path}"]},{"cell_type":"markdown","metadata":{"id":"09717HkzuXWR"},"source":["To evaluate the finetuned model on the first 50 test sentences in `test_manifest.json`, follow these steps:\n","\n","1. Ensure the checkpoint paths are correctly set in the copy of `inference.yaml` used for running inference.\n","2. Set up a model object.\n","3. Call `inference()` to generate predictions for the test subset.\n","4. Populate `pred_dict` with the generated inferences.\n","5. Compute the Word Error Rate (WER) for both the finetuned model and the pretrained model on the test subset of size 50.\n","6. Write down the WER for both models in `README.txt`.\n","\n","**Note**: Running inference on approximately 50 utterances might require around 15 minutes of computation on a Colab CPU. The code below uses CPU inference due to issues with checkpoint-loaded DNNs on SpeechBrain's GPU inference, but you can try it yourself.\n","\n","Replace `./results/finetuned_model/checkpoints` and `./results/pretrained_model/checkpoints` with the correct paths to the checkpoint directories for the finetuned and pretrained models respectively. Additionally, ensure you have a function `load_test_data` to load data from `test_manifest.json`. Finally, compute the WERs for both models and write them to `README.txt`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ylj89CJOsm91"},"outputs":[],"source":["device = 'cpu'\n","our_model = EncoderDecoderASR.from_hparams(\n","    source=ckpt_path, \n","    hparams_file='inference.yaml', \n","    savedir=\"our_ckpt\",\n","    run_opts={'device': device}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9j2daItrFBMA"},"outputs":[],"source":["### Populate test_manifest with the first 50 test sentences and then call inference() below###\n","EVAL_SIZE = 50\n","test_manifest = {\n","    k: v for k, v in list(test_manifest.items())[:EVAL_SIZE]\n","}\n","true_dict = {key: test_manifest[key]['words'] for key in test_manifest}\n","\n","pred_dict = inference(our_model.to(device), test_manifest)\n","# this data structure stores WER information we use later. \n","details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")\n","# word error rate (WER) summary using data structure we just created\n","sb.utils.edit_distance.wer_summary(details_by_utterance)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8VyYI2XtAwe"},"outputs":[],"source":["pred_dict = inference(crdnn, test_manifest)\n","# this data structure stores WER information we use later. \n","details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")\n","# word error rate (WER) summary using data structure we just created\n","sb.utils.edit_distance.wer_summary(details_by_utterance)"]},{"cell_type":"markdown","metadata":{"id":"p5PcGftQGwGo"},"source":["Training a sentiment detection model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFdcY0sWMYsi"},"outputs":[],"source":["# !gdown 1-s2e8dZYSjhVgfo_TL0V_89RZVhGnZ1Y #transcript.zip\n","!gdown 1oCn4PoJO-9XMEh-RtuatRgtKKL6ZyXb6\n","!unzip -q transcript.zip\n","\n","!gdown 1ChdI1XyhmGq9z8Y8M38yXPMob6oPRqPO  #train.txt\n","!gdown 10w15DnUbJcQRBSZWP03qjM6Oq8l7WSVQ  #dev.txt\n","\n","# !gdown 1eimo-BFXZz6Z3FeZK8uC-wT84ji-JVos #hvb-audio.zip\n","!gdown 1xMyXiFpQo3reF5sWFi6MahUQviW4Z1RA\n","!unzip -q hvb-audio.zip\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6gkwNE00zSn"},"outputs":[],"source":["with open('transcript/370981f1f0254ebc.json', 'r') as f:\n","    print(f.read())"]},{"cell_type":"markdown","metadata":{"id":"2jjw2l0WMq1f"},"source":["For sentiment classification on HVB utterances, follow these steps:\n","\n","1. Create train and dev splits from `train.txt` and `dev.txt`. Each line in these files represents a conversation-ID with sequential utterances. Use the provided text files to create train and dev sets, and extract the relevant \"emotion\" metadata from the corresponding transcript JSON files.\n","\n","2. Extract audio segments for the utterances using information from the transcript JSON files. The audio files are named after the conversation-ID within `/content/audio/agent` and `/content/audio/caller`. Utilize fields such as `channel_index`, `index`, `start_ms`, and `duration_ms` from the JSON files to extract the audio segments.\n","\n","3. Load the pretrained CRDNN Librispeech model and extract the encoder.\n","\n","4. Add a linear layer to map the audio signal encoding to sentiment predictions. This layer will be randomly initialized.\n","\n","5. Train both the new linear layer and all the pretrained encoder layers using cross-entropy loss with the reference emotion labels derived from the JSON files.\n","\n","6. Evaluate the trained sentiment detection model on utterances listed in `dev.txt` and compute the overall accuracy for sentiment prediction.\n","\n","7. Write down the accuracy obtained in `README.txt`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qV-JTx4-o3MY"},"outputs":[],"source":["!pip install pydub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xc6tENScx2Tp"},"outputs":[],"source":["import json\n","from pydub import AudioSegment\n","\n","train_data = []\n","train_audio_agent = []\n","train_audio_caller = []\n","with open('train.txt','r') as f:\n","    for line in f.readlines():\n","        convID = line.split(':')[0]\n","        indices = list(map(int,line.split(':')[1].split(',')))\n","        train_file = []\n","        with open(f'transcript/{convID}.json','r') as f1:\n","            utt = json.load(f1)\n","            for i in indices:\n","                train_obj = {}\n","                train_obj['words'] = utt[i - 1]['transcript']\n","                train_obj['emotion'] = list(utt[i - 1]['emotion'].values())\n","                train_obj['channel_index'] = utt[i - 1]['channel_index']\n","                train_obj['start_ms'] = utt[i - 1]['start_ms']\n","                train_obj['duration_ms'] = utt[i - 1]['duration_ms']\n","                train_obj['length'] = train_obj['duration_ms']/1000\n","                if train_obj['channel_index'] == 1:\n","                    path = f'audio/caller/'\n","                else:\n","                    path = f'audio/agent/'\n","                original_wav = AudioSegment.from_wav(path + str(convID) + '.wav')\n","                extracted_wav = original_wav[train_obj['start_ms']:train_obj['start_ms']+train_obj['duration_ms']]\n","                extracted_wav.export(path + str(convID) + '_' + str(i) + '.wav', format='wav')\n","                train_obj[\"wav\"] = path + str(convID) + '_' + str(i) + '.wav'\n","                train_file.append(train_obj)\n","        train_data.extend(train_file)\n","\n","dev_data = []\n","dev_audio_agent = []\n","dev_audio_caller = []\n","with open('dev.txt','r') as f:\n","    for line in f.readlines():\n","        convID = line.split(':')[0]\n","        indices = list(map(int,line.split(':')[1].split(',')))\n","        train_file = []\n","        with open(f'transcript/{convID}.json','r') as f1:\n","            utt = json.load(f1)\n","            for i in indices:\n","                train_obj = {}\n","                train_obj['words'] = utt[i - 1]['transcript']\n","                train_obj['emotion'] = list(utt[i - 1]['emotion'].values())\n","                train_obj['channel_index'] = utt[i - 1]['channel_index']\n","                train_obj['start_ms'] = utt[i - 1]['start_ms']\n","                train_obj['duration_ms'] = utt[i - 1]['duration_ms']\n","                train_obj['length'] = train_obj['duration_ms']/1000\n","                if train_obj['channel_index'] == 1:\n","                    path = f'audio/caller/'\n","                else:\n","                    path = f'audio/agent/'\n","                original_wav = AudioSegment.from_wav(path + str(convID) + '.wav')\n","                extracted_wav = original_wav[train_obj['start_ms']:train_obj['start_ms']+train_obj['duration_ms']]\n","                extracted_wav.export(path + str(convID) + '_' + str(i) + '_dev.wav', format='wav')\n","                train_obj[\"wav\"] = path + str(convID) + '_' + str(i) + '_dev.wav'\n","                train_file.append(train_obj)\n","        dev_data.extend(train_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6k-L4LGIwpW"},"outputs":[],"source":["crdnn = EncoderDecoderASR.from_hparams(\n","    source='speechbrain/asr-crdnn-rnnlm-librispeech',\n","    savedir='asr-crdnn-rnnlm-librispeech',\n","    run_opts={'device': 'cuda'}\n",")\n","\n","max_length = 0\n","\n","def getMaxLength(manifest):\n","    global max_length\n","    keys = list(manifest.keys())\n","    wav_paths = list(map(lambda x: x['wav'], manifest.values()))\n","    iterable = zip(keys, wav_paths)\n","    num_examples = len(manifest)\n","    for i in range(0, num_examples):\n","        batch_wavs = nn.utils.rnn.pad_sequence([\n","            torchaudio.load(path)[0].squeeze(0)\n","            for path in wav_paths[i:min(i + 1, num_examples)]\n","        ], batch_first=True)\n","        batch_keys = keys[i:min(i + 1, num_examples)]\n","        batch_wav_lens = torch.tensor([\n","            manifest[key]['length'] for key in batch_keys\n","        ])\n","        batch_wav_lens = batch_wav_lens / batch_wav_lens.max()\n","        max_length = max(max_length, batch_wavs.shape[1])\n","\n","train_manifest = {key: train_data[key] for key in range(len(train_data))}\n","\n","getMaxLength(train_manifest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jBd2Qkl-WSf"},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","class Network(nn.Module):\n","\n","    def __init__(self, crdnn):\n","        super().__init__()\n","        self.enc = crdnn.mods.encoder\n","        self.lin = nn.Linear(233*512,3)\n","    \n","    def forward(self, wav, wav_len):\n","        x = self.enc(wav,wav_len).reshape(1,-1)\n","        x = self.lin(x)\n","        return x\n","\n","model = Network(crdnn).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlQ60okRw2sr"},"outputs":[],"source":["# training\n","from tqdm.notebook import tqdm\n","\n","def mybatchify(manifest, batch_size):\n","    global max_length\n","    keys = list(manifest.keys())\n","    wav_paths = list(map(lambda x: x['wav'], manifest.values()))\n","    iterable = zip(keys, wav_paths)\n","    num_examples = len(manifest)\n","    for i in range(0, num_examples, batch_size):\n","        batch_wavs = nn.utils.rnn.pad_sequence([\n","            torchaudio.load(path)[0].squeeze(0)\n","            for path in wav_paths[i:min(i + batch_size, num_examples)]\n","        ], batch_first=True)\n","        batch_wavs = torch.cat([batch_wavs, torch.zeros(batch_wavs.size(0), max_length - batch_wavs.size(1))], dim=1)\n","        batch_keys = keys[i:min(i + batch_size, num_examples)]\n","        batch_wav_lens = torch.tensor([\n","            manifest[key]['length'] for key in batch_keys\n","        ])\n","        batch_wav_lens = batch_wav_lens / batch_wav_lens.max()\n","        emotions = torch.tensor([\n","            manifest[key]['emotion'] for key in batch_keys\n","        ])\n","        yield batch_keys, batch_wavs, batch_wav_lens, emotions\n","\n","def train(model, train_manifest, batch_size=8):\n","    torch.cuda.empty_cache()\n","    model.train()\n","    optim = torch.optim.Adam(model.parameters(),lr=0.0001,weight_decay=0.01)\n","    loss_fn = torch.nn.functional.cross_entropy\n","    total_loss = 0\n","    i = 0\n","    pred_dict = {}\n","    for keys, wavs, wav_lens, emotions in tqdm(mybatchify(train_manifest, batch_size), total=round(len(train_manifest) / batch_size + 0.5)):\n","        optim.zero_grad()\n","        preds = model(wavs.to(device), wav_lens.to(device))\n","        emotions = emotions.to(device)\n","        max_emotion = emotions // emotions.max()\n","        loss = torch.nn.functional.cross_entropy(preds,max_emotion)\n","        # if i % 20 == 0:\n","        #     print(loss.item())\n","        i += 1\n","        total_loss += loss.item()\n","        loss.backward()\n","        optim.step()\n","\n","train(model,train_manifest,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAjpfugWtQ8u"},"outputs":[],"source":["### Populate test_manifest with the first 50 test sentences and then call inference() below###\n","def evaluate(model, test_manifest, batch_size=8):\n","    torch.cuda.empty_cache()\n","    model.eval()\n","    pred_dict = {}\n","    score = 0\n","    for keys, wavs, wav_lens, emotions in tqdm(mybatchify(test_manifest, batch_size), total=round(len(test_manifest) / batch_size + 0.5)):\n","        preds = model(wavs.to(device), wav_lens.to(device))\n","        emotions = emotions.to(device)\n","        for i in range(preds.shape[0]):\n","            if torch.argmax(preds[i]) == torch.argmax(emotions[i]):\n","                score += 1\n","    print(score / len(test_manifest))\n","\n","test_manifest = {key: dev_data[key] for key in range(len(dev_data))}\n","\n","evaluate(model, test_manifest,1)"]},{"cell_type":"markdown","metadata":{"id":"qVTg1u4xM-Ez"},"source":["# Using Whisper's pretrained model to evaluate HVB"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibeWcRNRCXwW"},"outputs":[],"source":["!pip install openai-whisper"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hIC3SBBzxtlS"},"outputs":[],"source":["import whisper\n","\n","TEST_SIZE = 200\n","\n","with open('data/test_manifest.json', 'r') as f:\n","    test_manifest = json.load(f)\n","test_manifest = {\n","    k: v for k, v in list(test_manifest.items())[:TEST_SIZE]\n","}\n","true_dict = {key: test_manifest[key]['words'] for key in test_manifest}\n","\n","whisp = whisper.load_model('small').to('cuda')\n","\n","pred_dict = {}\n","for key in test_manifest:\n","    audio = test_manifest[key]['wav']\n","    result = whisp.transcribe(audio)\n","    pred_dict[key] = result['text'].upper()\n","\n","# this data structure stores WER information we use later. \n","details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")\n","# word error rate (WER) summary using data structure we just created\n","sb.utils.edit_distance.wer_summary(details_by_utterance)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"196b1E_4SievYhi8mxdoS10i72K--ySaK","timestamp":1680794530744}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
